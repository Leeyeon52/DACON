{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bdd79d0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 1 Physical GPUs, 1 Logical GPUs available\n",
      "[INFO] YOLO GPU not available, using CPU\n",
      "ì°¨ëŸ‰ ì™¸ê´€ ì´ìƒì¹˜ í•„í„°ë§ ì‹œì‘...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "í´ë˜ìŠ¤ë³„ í•„í„°ë§: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 396/396 [31:13<00:00,  4.73s/it]\n",
      "c:\\Users\\user\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\model_selection\\_split.py:805: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” ì „ì²´ ì´ë¯¸ì§€ ìˆ˜: 33137\n",
      "âœ… í•„í„°ë§ëœ ì°¨ëŸ‰ ì™¸ê´€ ì´ë¯¸ì§€ ìˆ˜: 13582\n",
      "ğŸ§¼ ì´ìƒì¹˜ ì œê±° ì™„ë£Œ. ì •ì œ ë¹„ìœ¨: 40.99%\n",
      "\n",
      "### Fold 1 ì‹œì‘ ###\n",
      "Epoch 1/15\n",
      "453/453 [==============================] - 303s 204ms/step - loss: 4.2333 - accuracy: 0.2354 - val_loss: 4.0540 - val_accuracy: 0.2367\n",
      "Epoch 2/15\n",
      "453/453 [==============================] - 41s 85ms/step - loss: 1.7778 - accuracy: 0.5934 - val_loss: 2.2768 - val_accuracy: 0.4280\n",
      "Epoch 3/15\n",
      "453/453 [==============================] - 40s 85ms/step - loss: 0.7636 - accuracy: 0.8117 - val_loss: 1.4227 - val_accuracy: 0.6091\n",
      "Epoch 4/15\n",
      "453/453 [==============================] - 40s 84ms/step - loss: 0.3909 - accuracy: 0.9080 - val_loss: 1.0578 - val_accuracy: 0.7052\n",
      "Epoch 5/15\n",
      "453/453 [==============================] - 41s 85ms/step - loss: 0.2412 - accuracy: 0.9412 - val_loss: 0.8703 - val_accuracy: 0.7589\n",
      "Epoch 6/15\n",
      "453/453 [==============================] - 41s 86ms/step - loss: 0.1707 - accuracy: 0.9624 - val_loss: 0.7680 - val_accuracy: 0.7943\n",
      "Epoch 7/15\n",
      "453/453 [==============================] - 41s 85ms/step - loss: 0.1396 - accuracy: 0.9686 - val_loss: 0.6648 - val_accuracy: 0.8204\n",
      "Epoch 8/15\n",
      "453/453 [==============================] - 40s 85ms/step - loss: 0.1013 - accuracy: 0.9794 - val_loss: 0.7988 - val_accuracy: 0.8101\n",
      "Epoch 9/15\n",
      "453/453 [==============================] - 41s 85ms/step - loss: 0.0957 - accuracy: 0.9777 - val_loss: 0.7254 - val_accuracy: 0.8204\n",
      "Epoch 10/15\n",
      "453/453 [==============================] - 41s 85ms/step - loss: 0.0760 - accuracy: 0.9845 - val_loss: 0.6588 - val_accuracy: 0.8403\n",
      "Epoch 11/15\n",
      "453/453 [==============================] - 40s 84ms/step - loss: 0.0738 - accuracy: 0.9827 - val_loss: 0.6741 - val_accuracy: 0.8311\n",
      "Epoch 12/15\n",
      "453/453 [==============================] - 41s 85ms/step - loss: 0.0689 - accuracy: 0.9845 - val_loss: 0.6691 - val_accuracy: 0.8425\n",
      "Epoch 13/15\n",
      "453/453 [==============================] - 41s 85ms/step - loss: 0.0669 - accuracy: 0.9848 - val_loss: 0.7081 - val_accuracy: 0.8384\n",
      "Epoch 14/15\n",
      "453/453 [==============================] - 41s 85ms/step - loss: 0.0643 - accuracy: 0.9838 - val_loss: 0.6870 - val_accuracy: 0.8487\n",
      "Epoch 15/15\n",
      "453/453 [==============================] - 41s 85ms/step - loss: 0.0592 - accuracy: 0.9857 - val_loss: 0.6331 - val_accuracy: 0.8557\n",
      "\n",
      "### Fold 2 ì‹œì‘ ###\n",
      "Epoch 1/15\n",
      "453/453 [==============================] - 46s 87ms/step - loss: 4.1725 - accuracy: 0.2489 - val_loss: 3.9121 - val_accuracy: 0.2482\n",
      "Epoch 2/15\n",
      "453/453 [==============================] - 41s 87ms/step - loss: 1.7470 - accuracy: 0.5936 - val_loss: 2.3266 - val_accuracy: 0.4197\n",
      "Epoch 3/15\n",
      "453/453 [==============================] - 41s 86ms/step - loss: 0.7409 - accuracy: 0.8217 - val_loss: 1.4569 - val_accuracy: 0.5976\n",
      "Epoch 4/15\n",
      "453/453 [==============================] - 41s 87ms/step - loss: 0.3951 - accuracy: 0.9059 - val_loss: 1.1803 - val_accuracy: 0.6719\n",
      "Epoch 5/15\n",
      "453/453 [==============================] - 42s 87ms/step - loss: 0.2371 - accuracy: 0.9435 - val_loss: 0.9627 - val_accuracy: 0.7390\n",
      "Epoch 6/15\n",
      "453/453 [==============================] - 42s 87ms/step - loss: 0.1679 - accuracy: 0.9613 - val_loss: 0.7791 - val_accuracy: 0.7868\n",
      "Epoch 7/15\n",
      "453/453 [==============================] - 42s 87ms/step - loss: 0.1365 - accuracy: 0.9669 - val_loss: 0.7739 - val_accuracy: 0.7890\n",
      "Epoch 8/15\n",
      "453/453 [==============================] - 41s 86ms/step - loss: 0.1134 - accuracy: 0.9734 - val_loss: 0.7421 - val_accuracy: 0.8082\n",
      "Epoch 9/15\n",
      "453/453 [==============================] - 41s 86ms/step - loss: 0.0954 - accuracy: 0.9787 - val_loss: 0.7330 - val_accuracy: 0.8093\n",
      "Epoch 10/15\n",
      "453/453 [==============================] - 41s 86ms/step - loss: 0.0878 - accuracy: 0.9801 - val_loss: 0.7563 - val_accuracy: 0.8074\n",
      "Epoch 11/15\n",
      "453/453 [==============================] - 41s 86ms/step - loss: 0.0772 - accuracy: 0.9826 - val_loss: 0.6696 - val_accuracy: 0.8317\n",
      "Epoch 12/15\n",
      "453/453 [==============================] - 41s 86ms/step - loss: 0.0738 - accuracy: 0.9832 - val_loss: 0.7332 - val_accuracy: 0.8185\n",
      "Epoch 13/15\n",
      "453/453 [==============================] - 42s 89ms/step - loss: 0.0600 - accuracy: 0.9857 - val_loss: 0.7108 - val_accuracy: 0.8395\n",
      "Epoch 14/15\n",
      "453/453 [==============================] - 42s 89ms/step - loss: 0.0562 - accuracy: 0.9867 - val_loss: 0.7450 - val_accuracy: 0.8258\n",
      "Epoch 15/15\n",
      "453/453 [==============================] - 43s 89ms/step - loss: 0.0589 - accuracy: 0.9867 - val_loss: 0.7285 - val_accuracy: 0.8395\n",
      "\n",
      "### Fold 3 ì‹œì‘ ###\n",
      "Epoch 1/15\n",
      "453/453 [==============================] - 68s 120ms/step - loss: 4.1998 - accuracy: 0.2480 - val_loss: 4.1807 - val_accuracy: 0.2286\n",
      "Epoch 2/15\n",
      "453/453 [==============================] - 57s 117ms/step - loss: 1.7905 - accuracy: 0.5916 - val_loss: 2.1784 - val_accuracy: 0.4613\n",
      "Epoch 3/15\n",
      "453/453 [==============================] - 51s 104ms/step - loss: 0.7597 - accuracy: 0.8174 - val_loss: 1.3447 - val_accuracy: 0.6314\n",
      "Epoch 4/15\n",
      "453/453 [==============================] - 42s 87ms/step - loss: 0.3783 - accuracy: 0.9116 - val_loss: 1.0239 - val_accuracy: 0.7051\n",
      "Epoch 5/15\n",
      "453/453 [==============================] - 42s 88ms/step - loss: 0.2339 - accuracy: 0.9465 - val_loss: 0.9921 - val_accuracy: 0.7378\n",
      "Epoch 6/15\n",
      "453/453 [==============================] - 41s 86ms/step - loss: 0.1650 - accuracy: 0.9607 - val_loss: 0.8658 - val_accuracy: 0.7662\n",
      "Epoch 7/15\n",
      "453/453 [==============================] - 41s 87ms/step - loss: 0.1283 - accuracy: 0.9702 - val_loss: 0.8273 - val_accuracy: 0.7813\n",
      "Epoch 8/15\n",
      "453/453 [==============================] - 44s 91ms/step - loss: 0.0977 - accuracy: 0.9799 - val_loss: 0.6370 - val_accuracy: 0.8332\n",
      "Epoch 9/15\n",
      "453/453 [==============================] - 42s 86ms/step - loss: 0.0885 - accuracy: 0.9798 - val_loss: 0.6714 - val_accuracy: 0.8281\n",
      "Epoch 10/15\n",
      "453/453 [==============================] - 41s 86ms/step - loss: 0.0874 - accuracy: 0.9811 - val_loss: 0.6462 - val_accuracy: 0.8328\n",
      "Epoch 11/15\n",
      "453/453 [==============================] - 41s 87ms/step - loss: 0.0686 - accuracy: 0.9833 - val_loss: 0.5737 - val_accuracy: 0.8406\n",
      "Epoch 12/15\n",
      "453/453 [==============================] - 42s 87ms/step - loss: 0.0632 - accuracy: 0.9868 - val_loss: 0.6944 - val_accuracy: 0.8299\n",
      "Epoch 13/15\n",
      "453/453 [==============================] - 42s 87ms/step - loss: 0.0659 - accuracy: 0.9835 - val_loss: 0.6365 - val_accuracy: 0.8443\n",
      "Epoch 14/15\n",
      "453/453 [==============================] - 42s 87ms/step - loss: 0.0634 - accuracy: 0.9851 - val_loss: 0.5730 - val_accuracy: 0.8586\n",
      "Epoch 15/15\n",
      "453/453 [==============================] - 42s 87ms/step - loss: 0.0523 - accuracy: 0.9885 - val_loss: 0.6668 - val_accuracy: 0.8387\n",
      "\n",
      "### Fold 4 ì‹œì‘ ###\n",
      "Epoch 1/15\n",
      "453/453 [==============================] - 48s 90ms/step - loss: 4.2516 - accuracy: 0.2359 - val_loss: 4.2675 - val_accuracy: 0.2367\n",
      "Epoch 2/15\n",
      "453/453 [==============================] - 42s 87ms/step - loss: 1.7852 - accuracy: 0.5953 - val_loss: 2.2028 - val_accuracy: 0.4429\n",
      "Epoch 3/15\n",
      "453/453 [==============================] - 43s 90ms/step - loss: 0.7516 - accuracy: 0.8171 - val_loss: 1.4367 - val_accuracy: 0.6016\n",
      "Epoch 4/15\n",
      "453/453 [==============================] - 45s 94ms/step - loss: 0.3962 - accuracy: 0.9055 - val_loss: 0.9886 - val_accuracy: 0.7187\n",
      "Epoch 5/15\n",
      "453/453 [==============================] - 43s 89ms/step - loss: 0.2399 - accuracy: 0.9467 - val_loss: 0.8650 - val_accuracy: 0.7570\n",
      "Epoch 6/15\n",
      "453/453 [==============================] - 42s 88ms/step - loss: 0.1695 - accuracy: 0.9631 - val_loss: 0.7644 - val_accuracy: 0.7861\n",
      "Epoch 7/15\n",
      "453/453 [==============================] - 42s 87ms/step - loss: 0.1267 - accuracy: 0.9721 - val_loss: 0.7625 - val_accuracy: 0.7964\n",
      "Epoch 8/15\n",
      "453/453 [==============================] - 42s 88ms/step - loss: 0.1074 - accuracy: 0.9757 - val_loss: 0.6815 - val_accuracy: 0.8236\n",
      "Epoch 9/15\n",
      "453/453 [==============================] - 42s 88ms/step - loss: 0.0896 - accuracy: 0.9820 - val_loss: 0.7002 - val_accuracy: 0.8270\n",
      "Epoch 10/15\n",
      "453/453 [==============================] - 42s 87ms/step - loss: 0.0826 - accuracy: 0.9814 - val_loss: 0.7946 - val_accuracy: 0.8104\n",
      "Epoch 11/15\n",
      "453/453 [==============================] - 43s 90ms/step - loss: 0.0791 - accuracy: 0.9822 - val_loss: 0.7512 - val_accuracy: 0.8071\n",
      "Epoch 12/15\n",
      "453/453 [==============================] - 44s 91ms/step - loss: 0.0775 - accuracy: 0.9809 - val_loss: 0.6687 - val_accuracy: 0.8443\n",
      "Epoch 13/15\n",
      "453/453 [==============================] - 43s 89ms/step - loss: 0.0731 - accuracy: 0.9831 - val_loss: 0.6229 - val_accuracy: 0.8476\n",
      "Epoch 14/15\n",
      "453/453 [==============================] - 42s 87ms/step - loss: 0.0616 - accuracy: 0.9855 - val_loss: 0.5694 - val_accuracy: 0.8634\n",
      "Epoch 15/15\n",
      "453/453 [==============================] - 42s 88ms/step - loss: 0.0536 - accuracy: 0.9878 - val_loss: 0.5323 - val_accuracy: 0.8719\n",
      "\n",
      "### Fold 5 ì‹œì‘ ###\n",
      "Epoch 1/15\n",
      "453/453 [==============================] - 47s 89ms/step - loss: 4.2063 - accuracy: 0.2396 - val_loss: 4.1965 - val_accuracy: 0.2135\n",
      "Epoch 2/15\n",
      "453/453 [==============================] - 41s 85ms/step - loss: 1.7858 - accuracy: 0.5902 - val_loss: 2.1721 - val_accuracy: 0.4392\n",
      "Epoch 3/15\n",
      "453/453 [==============================] - 41s 87ms/step - loss: 0.7749 - accuracy: 0.8108 - val_loss: 1.4273 - val_accuracy: 0.6024\n",
      "Epoch 4/15\n",
      "453/453 [==============================] - 44s 92ms/step - loss: 0.4030 - accuracy: 0.9037 - val_loss: 1.0627 - val_accuracy: 0.7043\n",
      "Epoch 5/15\n",
      "453/453 [==============================] - 75s 160ms/step - loss: 0.2528 - accuracy: 0.9372 - val_loss: 0.9439 - val_accuracy: 0.7375\n",
      "Epoch 6/15\n",
      "453/453 [==============================] - 41s 86ms/step - loss: 0.1698 - accuracy: 0.9597 - val_loss: 0.7865 - val_accuracy: 0.7842\n",
      "Epoch 7/15\n",
      "453/453 [==============================] - 41s 86ms/step - loss: 0.1335 - accuracy: 0.9692 - val_loss: 0.6953 - val_accuracy: 0.8067\n",
      "Epoch 8/15\n",
      "453/453 [==============================] - 41s 86ms/step - loss: 0.1065 - accuracy: 0.9763 - val_loss: 0.7299 - val_accuracy: 0.8063\n",
      "Epoch 9/15\n",
      "453/453 [==============================] - 41s 86ms/step - loss: 0.0987 - accuracy: 0.9758 - val_loss: 0.7572 - val_accuracy: 0.7979\n",
      "Epoch 10/15\n",
      "453/453 [==============================] - 41s 86ms/step - loss: 0.0898 - accuracy: 0.9800 - val_loss: 0.6215 - val_accuracy: 0.8501\n",
      "Epoch 11/15\n",
      "453/453 [==============================] - 41s 86ms/step - loss: 0.0747 - accuracy: 0.9830 - val_loss: 0.6920 - val_accuracy: 0.8321\n",
      "Epoch 12/15\n",
      "453/453 [==============================] - 41s 86ms/step - loss: 0.0723 - accuracy: 0.9834 - val_loss: 0.7601 - val_accuracy: 0.8207\n",
      "Epoch 13/15\n",
      "453/453 [==============================] - 41s 87ms/step - loss: 0.0740 - accuracy: 0.9839 - val_loss: 0.6674 - val_accuracy: 0.8490\n",
      "Epoch 14/15\n",
      "453/453 [==============================] - 41s 87ms/step - loss: 0.0602 - accuracy: 0.9873 - val_loss: 0.5072 - val_accuracy: 0.8726\n",
      "Epoch 15/15\n",
      "453/453 [==============================] - 42s 87ms/step - loss: 0.0558 - accuracy: 0.9858 - val_loss: 0.5237 - val_accuracy: 0.8634\n",
      "í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¶”ë¡  ì‹œì‘...\n",
      "345/345 [==============================] - 24s 67ms/step\n",
      "ì¶”ë¡  ì™„ë£Œ\n",
      "submission.csv ì €ì¥ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.applications.efficientnet import preprocess_input\n",
    "\n",
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "import torch\n",
    "\n",
    "# -----------------------\n",
    "# TensorFlow GPU ì„¤ì •\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "        print(f\"[INFO] {len(gpus)} Physical GPUs, {len(logical_gpus)} Logical GPUs available\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "else:\n",
    "    print(\"[INFO] No GPU detected for TensorFlow\")\n",
    "\n",
    "# -----------------------\n",
    "# Config\n",
    "CFG = {\n",
    "    'IMG_SIZE': 224,\n",
    "    'EPOCHS': 15,\n",
    "    'LR': 3e-4,\n",
    "    'BATCH_SIZE': 24,\n",
    "    'SEED': 2025,\n",
    "    'FOLDS': 5\n",
    "}\n",
    "\n",
    "# Seed ê³ ì •\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "\n",
    "set_seed(CFG['SEED'])\n",
    "\n",
    "# -----------------------\n",
    "# ê²½ë¡œ ì„¤ì •\n",
    "ORIGINAL_TRAIN_DIR = \"D:/ë°ì´ì½˜ 250519 ëŒ€íšŒ/open/train\"\n",
    "FILTERED_TRAIN_DIR = \"D:/ë°ì´ì½˜ 250519 ëŒ€íšŒ/filtered_train\"\n",
    "TEST_DIR = \"D:/ë°ì´ì½˜ 250519 ëŒ€íšŒ/open/test\"\n",
    "SAMPLE_SUB = \"D:/ë°ì´ì½˜ 250519 ëŒ€íšŒ/open/sample_submission.csv\"\n",
    "\n",
    "# -----------------------\n",
    "# Ultralytics YOLO GPU ì„¤ì • ë° ì°¨ëŸ‰ í´ë˜ìŠ¤ ì •ì˜\n",
    "VEHICLE_CLASSES = [2, 5, 7]  # car, bus, truck í´ë˜ìŠ¤ ë²ˆí˜¸\n",
    "\n",
    "yolo_model = YOLO('yolov8n.pt')\n",
    "if torch.cuda.is_available():\n",
    "    yolo_model.model = yolo_model.model.cuda()\n",
    "    print(\"[INFO] YOLO model moved to GPU\")\n",
    "else:\n",
    "    print(\"[INFO] YOLO GPU not available, using CPU\")\n",
    "\n",
    "# -----------------------\n",
    "# ì°¨ëŸ‰ ì™¸ê´€ ì „ì²´ í¬í•¨ ë° ë³´ë‹›/íŠ¸ë í¬ ì—´ë¦° ì´ìƒì¹˜ í•„í„°ë§ í•¨ìˆ˜\n",
    "def is_full_vehicle(detections, img):\n",
    "    h, w = img.shape[:2]\n",
    "    img_area = h * w\n",
    "\n",
    "    for det in detections:\n",
    "        cls = int(det.cls)\n",
    "        if cls not in VEHICLE_CLASSES:\n",
    "            continue\n",
    "\n",
    "        x1, y1, x2, y2 = det.xyxy[0].cpu().numpy()\n",
    "        box_w, box_h = x2 - x1, y2 - y1\n",
    "        area = box_w * box_h\n",
    "        center_x, center_y = (x1 + x2) / 2, (y1 + y2) / 2\n",
    "        aspect_ratio = box_w / box_h\n",
    "\n",
    "        # ì°¨ëŸ‰ ì¤‘ì•™ ê·¼ì²˜ ìœ„ì¹˜ ì¡°ê±´ (ì¤‘ì•™ 30% ì˜ì—­ ë‚´)\n",
    "        if not (0.35 * w < center_x < 0.65 * w and 0.35 * h < center_y < 0.65 * h):\n",
    "            continue\n",
    "\n",
    "        # ì°¨ëŸ‰ ë°•ìŠ¤ í¬ê¸° ìµœì†Œ ë¹„ìœ¨ ì¡°ê±´ (ì „ì²´ ì´ë¯¸ì§€ì˜ 35% ì´ìƒ)\n",
    "        if area / img_area < 0.35:\n",
    "            continue\n",
    "\n",
    "        # ì°¨ëŸ‰ ë¹„ìœ¨ ì¡°ê±´ (ë„ˆë¬´ ê¸¸ê±°ë‚˜ ë„ˆë¬´ ì¢ìœ¼ë©´ ì œê±° - íŠ¸ë í¬/ë³´ë‹› ì—´ë¦¼ ë°©ì§€)\n",
    "        if aspect_ratio < 0.9 or aspect_ratio > 2.3:\n",
    "            continue\n",
    "\n",
    "        # ë³´ë‹›/íŠ¸ë í¬ ì—´ë¦° ì°¨ëŸ‰ì€ ë°•ìŠ¤ ë‚´ë¶€ ì˜ì—­ íŒ¨í„´ ë¶„ì„ ì¶”ê°€\n",
    "        # (ì˜ˆ: ë°•ìŠ¤ ìƒë‹¨ ë˜ëŠ” í•˜ë‹¨ì— ì§€ë‚˜ì¹˜ê²Œ í° ë¹ˆ ì˜ì—­ì´ ìˆëŠ”ì§€ ê°„ë‹¨ ì²´í¬)\n",
    "        # ì´ë¯¸ì§€ ì˜ì—­ crop\n",
    "        vehicle_crop = img[int(y1):int(y2), int(x1):int(x2)]\n",
    "        gray = cv2.cvtColor(vehicle_crop, cv2.COLOR_BGR2GRAY)\n",
    "        _, thresh = cv2.threshold(gray, 200, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "        # ìƒë‹¨ 10% ì˜ì—­ì— í°ìƒ‰(ì—´ë¦° ë³´ë‹› ë“±) í”½ì…€ ë¹„ìœ¨ ì²´í¬\n",
    "        top_area = thresh[0:int(0.1 * thresh.shape[0]), :]\n",
    "        white_ratio_top = np.sum(top_area == 255) / top_area.size\n",
    "\n",
    "        # í•˜ë‹¨ 10% ì˜ì—­ ì²´í¬ (íŠ¸ë í¬ ì—´ë¦¼ ê°€ëŠ¥ì„±)\n",
    "        bottom_area = thresh[int(0.9 * thresh.shape[0]):, :]\n",
    "        white_ratio_bottom = np.sum(bottom_area == 255) / bottom_area.size\n",
    "\n",
    "        # ë„ˆë¬´ í° ì˜ì—­ì´ ë§ìœ¼ë©´ ë³´ë‹›/íŠ¸ë í¬ ì—´ë¦° ì´ìƒì¹˜ë¡œ ê°„ì£¼\n",
    "        if white_ratio_top > 0.25 or white_ratio_bottom > 0.25:\n",
    "            continue\n",
    "\n",
    "        # ìœ„ ì¡°ê±´ ëª¨ë‘ í†µê³¼í•˜ë©´ ì •ìƒ ì°¨ëŸ‰ ì™¸ê´€ í¬í•¨ìœ¼ë¡œ íŒë‹¨\n",
    "        return True\n",
    "\n",
    "    return False\n",
    "\n",
    "# -----------------------\n",
    "# ì°¨ëŸ‰ ì™¸ê´€ ì´ìƒì¹˜ í•„í„°ë§ ìˆ˜í–‰\n",
    "print(\"ì°¨ëŸ‰ ì™¸ê´€ ì´ìƒì¹˜ í•„í„°ë§ ì‹œì‘...\")\n",
    "os.makedirs(FILTERED_TRAIN_DIR, exist_ok=True)\n",
    "class_dirs = sorted(os.listdir(ORIGINAL_TRAIN_DIR))\n",
    "\n",
    "filtered_count = 0\n",
    "total_count = 0\n",
    "\n",
    "for cls in tqdm(class_dirs, desc=\"í´ë˜ìŠ¤ë³„ í•„í„°ë§\"):\n",
    "    input_dir = os.path.join(ORIGINAL_TRAIN_DIR, cls)\n",
    "    output_dir = os.path.join(FILTERED_TRAIN_DIR, cls)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    image_paths = glob(os.path.join(input_dir, \"*.jpg\"))\n",
    "    for img_path in image_paths:\n",
    "        total_count += 1\n",
    "        img = cv2.imread(img_path)\n",
    "        results = yolo_model(img, verbose=False)[0]\n",
    "\n",
    "        if is_full_vehicle(results.boxes, img):\n",
    "            filename = os.path.basename(img_path)\n",
    "            cv2.imwrite(os.path.join(output_dir, filename), img)\n",
    "            filtered_count += 1\n",
    "\n",
    "print(f\"ğŸ” ì „ì²´ ì´ë¯¸ì§€ ìˆ˜: {total_count}\")\n",
    "print(f\"âœ… í•„í„°ë§ëœ ì°¨ëŸ‰ ì™¸ê´€ ì´ë¯¸ì§€ ìˆ˜: {filtered_count}\")\n",
    "print(f\"ğŸ§¼ ì´ìƒì¹˜ ì œê±° ì™„ë£Œ. ì •ì œ ë¹„ìœ¨: {filtered_count / total_count:.2%}\")\n",
    "\n",
    "# -----------------------\n",
    "# í•™ìŠµ ë°ì´í„° ì¤€ë¹„\n",
    "label_list = sorted(os.listdir(FILTERED_TRAIN_DIR))\n",
    "label2id = {v: i for i, v in enumerate(label_list)}\n",
    "id2label = {i: v for v, i in label2id.items()}\n",
    "\n",
    "image_paths = glob(os.path.join(FILTERED_TRAIN_DIR, '*', '*.jpg'))\n",
    "labels = [label2id[os.path.basename(os.path.dirname(p))] for p in image_paths]\n",
    "\n",
    "# -----------------------\n",
    "# ë°ì´í„° ì „ì²˜ë¦¬ í•¨ìˆ˜\n",
    "def load_and_preprocess(img_path):\n",
    "    img = load_img(img_path, target_size=(CFG['IMG_SIZE'], CFG['IMG_SIZE']))\n",
    "    img = img_to_array(img)\n",
    "    img = preprocess_input(img)\n",
    "    return img\n",
    "\n",
    "def create_dataset(image_paths, labels=None, is_train=True):\n",
    "    def gen():\n",
    "        for i, path in enumerate(image_paths):\n",
    "            img = load_and_preprocess(path)\n",
    "            if labels is not None:\n",
    "                yield img, labels[i]\n",
    "            else:\n",
    "                yield img\n",
    "\n",
    "    if labels is not None:\n",
    "        ds = tf.data.Dataset.from_generator(\n",
    "            gen,\n",
    "            output_types=(tf.float32, tf.int32),\n",
    "            output_shapes=((CFG['IMG_SIZE'], CFG['IMG_SIZE'], 3), ())\n",
    "        )\n",
    "    else:\n",
    "        ds = tf.data.Dataset.from_generator(\n",
    "            gen,\n",
    "            output_types=tf.float32,\n",
    "            output_shapes=(CFG['IMG_SIZE'], CFG['IMG_SIZE'], 3)\n",
    "        )\n",
    "\n",
    "    if is_train:\n",
    "        ds = ds.shuffle(1024)\n",
    "    ds = ds.batch(CFG['BATCH_SIZE']).prefetch(tf.data.AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "# -----------------------\n",
    "# ëª¨ë¸ ìƒì„±\n",
    "def build_model(num_classes):\n",
    "    base = tf.keras.applications.EfficientNetB0(\n",
    "        include_top=False,\n",
    "        input_shape=(CFG['IMG_SIZE'], CFG['IMG_SIZE'], 3),\n",
    "        weights='imagenet',\n",
    "        pooling='avg'\n",
    "    )\n",
    "    x = layers.Dense(num_classes, activation='softmax')(base.output)\n",
    "    model = models.Model(inputs=base.input, outputs=x)\n",
    "    return model\n",
    "\n",
    "# -----------------------\n",
    "# Stratified K-Fold í•™ìŠµ\n",
    "skf = StratifiedKFold(n_splits=CFG['FOLDS'], shuffle=True, random_state=CFG['SEED'])\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(image_paths, labels)):\n",
    "    print(f\"\\n### Fold {fold+1} ì‹œì‘ ###\")\n",
    "\n",
    "    train_paths = [image_paths[i] for i in train_idx]\n",
    "    val_paths = [image_paths[i] for i in val_idx]\n",
    "    train_labels = [labels[i] for i in train_idx]\n",
    "    val_labels = [labels[i] for i in val_idx]\n",
    "\n",
    "    train_ds = create_dataset(train_paths, train_labels, is_train=True)\n",
    "    val_ds = create_dataset(val_paths, val_labels, is_train=False)\n",
    "\n",
    "    model = build_model(num_classes=len(label2id))\n",
    "    model.compile(\n",
    "        optimizer=optimizers.Adam(CFG['LR']),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    model.fit(\n",
    "        train_ds,\n",
    "        validation_data=val_ds,\n",
    "        epochs=CFG['EPOCHS'],\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "# -----------------------\n",
    "# í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¶”ë¡  ë° ì œì¶œ íŒŒì¼ ìƒì„±\n",
    "test_paths = sorted(glob(os.path.join(TEST_DIR, '*.jpg')))\n",
    "test_ds = create_dataset(test_paths, is_train=False)\n",
    "\n",
    "print(\"í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¶”ë¡  ì‹œì‘...\")\n",
    "preds = model.predict(test_ds)\n",
    "print(\"ì¶”ë¡  ì™„ë£Œ\")\n",
    "\n",
    "submission = pd.read_csv(SAMPLE_SUB)\n",
    "for idx, class_name in enumerate(label2id.keys()):\n",
    "    submission[class_name] = preds[:, idx]\n",
    "\n",
    "submission.to_csv(\"submission.csv\", index=False)\n",
    "print(\"submission.csv ì €ì¥ ì™„ë£Œ\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
