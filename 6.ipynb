{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b855f9e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "í´ë˜ìŠ¤ë³„ í•„í„°ë§: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 396/396 [34:01<00:00,  5.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì „ì²´ ì´ë¯¸ì§€ ìˆ˜: 33137\n",
      "í•„í„°ë§ëœ ì°¨ëŸ‰ ì™¸ê´€ ì´ë¯¸ì§€ ìˆ˜: 31822\n",
      "ì •ì œ ë¹„ìœ¨: 96.03%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# ê²½ë¡œ ì„¤ì •\n",
    "TRAIN_DIR = \"D:/ë°ì´ì½˜ 250519 ëŒ€íšŒ/open/train\"\n",
    "FILTERED_DIR = \"D:/ë°ì´ì½˜ 250519 ëŒ€íšŒ/filtered_train\"\n",
    "\n",
    "# YOLOv8 ëª¨ë¸ ë¡œë“œ (ê°€ë²¼ìš´ yolov8n.pt ì‚¬ìš©)\n",
    "model = YOLO('yolov8n.pt')\n",
    "\n",
    "def is_full_vehicle(detections, img_shape):\n",
    "    h, w = img_shape[:2]\n",
    "\n",
    "    for det in detections:\n",
    "        cls = int(det.cls)\n",
    "        # ì°¨ëŸ‰ í´ë˜ìŠ¤ë§Œ í•„í„°ë§ (COCO ê¸°ì¤€: 2=car, 5=bus, 7=truck)\n",
    "        if cls in [2, 5, 7]:\n",
    "            x1, y1, x2, y2 = det.xyxy[0].cpu().numpy()\n",
    "            box_w = x2 - x1\n",
    "            box_h = y2 - y1\n",
    "            area = box_w * box_h\n",
    "            img_area = h * w\n",
    "            center_x = (x1 + x2) / 2\n",
    "            center_y = (y1 + y2) / 2\n",
    "            aspect_ratio = box_w / box_h\n",
    "\n",
    "            # 1) ì°¨ëŸ‰ì´ ì´ë¯¸ì§€ ì¤‘ì•™ ê·¼ì²˜ì— ìœ„ì¹˜ (ë„ˆë¬´ ì¹˜ìš°ì¹˜ë©´ ì œì™¸)\n",
    "            if not (0.35*w < center_x < 0.65*w and 0.35*h < center_y < 0.65*h):\n",
    "                continue\n",
    "\n",
    "            # 2) ì°¨ëŸ‰ ë°•ìŠ¤ê°€ ì´ë¯¸ì§€ ëŒ€ë¹„ ì¶©ë¶„íˆ í° ë©´ì  (ì˜ˆ: 35% ì´ìƒ)\n",
    "            if area / img_area < 0.35:\n",
    "                continue\n",
    "\n",
    "            # 3) ì°¨ëŸ‰ ë°•ìŠ¤ ë¹„ìœ¨ í•„í„°ë§\n",
    "            if aspect_ratio < 0.8 or aspect_ratio > 2.5:\n",
    "                continue\n",
    "\n",
    "            # ì¡°ê±´ ëª¨ë‘ ë§Œì¡±í•˜ëŠ” ì°¨ëŸ‰ì´ ìˆìœ¼ë©´ True ë°˜í™˜\n",
    "            return True\n",
    "\n",
    "    return False\n",
    "\n",
    "# í•„í„°ë§ ìˆ˜í–‰\n",
    "os.makedirs(FILTERED_DIR, exist_ok=True)\n",
    "class_dirs = sorted(os.listdir(TRAIN_DIR))\n",
    "\n",
    "filtered_count = 0\n",
    "total_count = 0\n",
    "\n",
    "for cls in tqdm(class_dirs, desc=\"í´ë˜ìŠ¤ë³„ í•„í„°ë§\"):\n",
    "    input_dir = os.path.join(TRAIN_DIR, cls)\n",
    "    output_dir = os.path.join(FILTERED_DIR, cls)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    image_paths = glob(os.path.join(input_dir, \"*.jpg\"))\n",
    "    for img_path in image_paths:\n",
    "        total_count += 1\n",
    "        img = cv2.imread(img_path)\n",
    "        results = model(img, verbose=False)[0]\n",
    "\n",
    "        if is_full_vehicle(results.boxes, img.shape):\n",
    "            filename = os.path.basename(img_path)\n",
    "            cv2.imwrite(os.path.join(output_dir, filename), img)\n",
    "            filtered_count += 1\n",
    "\n",
    "print(f\"ì „ì²´ ì´ë¯¸ì§€ ìˆ˜: {total_count}\")\n",
    "print(f\"í•„í„°ë§ëœ ì°¨ëŸ‰ ì™¸ê´€ ì´ë¯¸ì§€ ìˆ˜: {filtered_count}\")\n",
    "print(f\"ì •ì œ ë¹„ìœ¨: {filtered_count / total_count:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f5a530e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 1 GPU(s) available: ['/physical_device:GPU:0']\n",
      "ì°¨ëŸ‰ ì™¸ê´€ ì´ìƒì¹˜ í•„í„°ë§ ì‹œì‘...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "í´ë˜ìŠ¤ë³„ í•„í„°ë§: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 396/396 [36:02<00:00,  5.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” ì „ì²´ ì´ë¯¸ì§€ ìˆ˜: 33137\n",
      "âœ… í•„í„°ë§ëœ ì°¨ëŸ‰ ì™¸ê´€ ì´ë¯¸ì§€ ìˆ˜: 31822\n",
      "ğŸ§¼ ì´ìƒì¹˜ ì œê±° ì™„ë£Œ. ì •ì œ ë¹„ìœ¨: 96.03%\n",
      "\n",
      "### Fold 1 ì‹œì‘ ###\n",
      "Epoch 1/15\n",
      "1054/1054 [==============================] - 237s 213ms/step - loss: 2.8560 - accuracy: 0.4437 - val_loss: 4.0846 - val_accuracy: 0.1942\n",
      "Epoch 2/15\n",
      "1054/1054 [==============================] - 91s 84ms/step - loss: 0.8990 - accuracy: 0.7484 - val_loss: 2.4110 - val_accuracy: 0.3700\n",
      "Epoch 3/15\n",
      "1054/1054 [==============================] - 91s 84ms/step - loss: 0.4149 - accuracy: 0.8765 - val_loss: 1.8808 - val_accuracy: 0.4949\n",
      "Epoch 4/15\n",
      "1054/1054 [==============================] - 91s 84ms/step - loss: 0.2615 - accuracy: 0.9217 - val_loss: 1.6234 - val_accuracy: 0.5501\n",
      "Epoch 5/15\n",
      "1054/1054 [==============================] - 90s 84ms/step - loss: 0.1895 - accuracy: 0.9412 - val_loss: 1.4452 - val_accuracy: 0.6045\n",
      "Epoch 6/15\n",
      "1054/1054 [==============================] - 90s 84ms/step - loss: 0.1505 - accuracy: 0.9555 - val_loss: 1.3274 - val_accuracy: 0.6525\n",
      "Epoch 7/15\n",
      "1054/1054 [==============================] - 90s 84ms/step - loss: 0.1301 - accuracy: 0.9610 - val_loss: 1.2378 - val_accuracy: 0.6912\n",
      "Epoch 8/15\n",
      "1054/1054 [==============================] - 90s 84ms/step - loss: 0.1047 - accuracy: 0.9699 - val_loss: 1.2122 - val_accuracy: 0.7150\n",
      "Epoch 9/15\n",
      "1054/1054 [==============================] - 90s 84ms/step - loss: 0.0948 - accuracy: 0.9712 - val_loss: 1.0863 - val_accuracy: 0.7368\n",
      "Epoch 10/15\n",
      "1054/1054 [==============================] - 90s 83ms/step - loss: 0.0748 - accuracy: 0.9778 - val_loss: 0.9767 - val_accuracy: 0.7501\n",
      "Epoch 11/15\n",
      "1054/1054 [==============================] - 90s 84ms/step - loss: 0.0690 - accuracy: 0.9815 - val_loss: 1.0666 - val_accuracy: 0.7316\n",
      "Epoch 12/15\n",
      "1054/1054 [==============================] - 90s 84ms/step - loss: 0.0674 - accuracy: 0.9816 - val_loss: 0.9430 - val_accuracy: 0.7595\n",
      "Epoch 13/15\n",
      "1054/1054 [==============================] - 90s 83ms/step - loss: 0.0612 - accuracy: 0.9829 - val_loss: 0.8638 - val_accuracy: 0.7925\n",
      "Epoch 14/15\n",
      "1054/1054 [==============================] - 90s 84ms/step - loss: 0.0560 - accuracy: 0.9846 - val_loss: 0.7746 - val_accuracy: 0.8082\n",
      "Epoch 15/15\n",
      "1054/1054 [==============================] - 90s 84ms/step - loss: 0.0488 - accuracy: 0.9864 - val_loss: 0.9867 - val_accuracy: 0.7955\n",
      "\n",
      "### Fold 2 ì‹œì‘ ###\n",
      "Epoch 1/15\n",
      "1054/1054 [==============================] - 97s 86ms/step - loss: 2.8323 - accuracy: 0.4535 - val_loss: 4.3194 - val_accuracy: 0.1904\n",
      "Epoch 2/15\n",
      "1054/1054 [==============================] - 91s 84ms/step - loss: 0.8997 - accuracy: 0.7529 - val_loss: 2.6095 - val_accuracy: 0.3440\n",
      "Epoch 3/15\n",
      "1054/1054 [==============================] - 93s 86ms/step - loss: 0.4120 - accuracy: 0.8771 - val_loss: 1.7429 - val_accuracy: 0.5118\n",
      "Epoch 4/15\n",
      "1054/1054 [==============================] - 94s 87ms/step - loss: 0.2582 - accuracy: 0.9217 - val_loss: 1.6264 - val_accuracy: 0.5438\n",
      "Epoch 5/15\n",
      "1054/1054 [==============================] - 94s 87ms/step - loss: 0.1865 - accuracy: 0.9430 - val_loss: 1.4771 - val_accuracy: 0.5925\n",
      "Epoch 6/15\n",
      "1054/1054 [==============================] - 94s 87ms/step - loss: 0.1528 - accuracy: 0.9528 - val_loss: 1.2055 - val_accuracy: 0.6795\n",
      "Epoch 7/15\n",
      "1054/1054 [==============================] - 94s 87ms/step - loss: 0.1255 - accuracy: 0.9628 - val_loss: 1.2027 - val_accuracy: 0.6685\n",
      "Epoch 8/15\n",
      "1054/1054 [==============================] - 126s 118ms/step - loss: 0.1030 - accuracy: 0.9702 - val_loss: 1.2053 - val_accuracy: 0.6973\n",
      "Epoch 9/15\n",
      "1054/1054 [==============================] - 141s 131ms/step - loss: 0.0911 - accuracy: 0.9738 - val_loss: 1.2689 - val_accuracy: 0.6808\n",
      "Epoch 10/15\n",
      "1054/1054 [==============================] - 141s 130ms/step - loss: 0.0772 - accuracy: 0.9781 - val_loss: 1.3533 - val_accuracy: 0.6775\n",
      "Epoch 11/15\n",
      "1054/1054 [==============================] - 104s 95ms/step - loss: 0.0746 - accuracy: 0.9795 - val_loss: 1.0310 - val_accuracy: 0.7371\n",
      "Epoch 12/15\n",
      "1054/1054 [==============================] - 92s 86ms/step - loss: 0.0563 - accuracy: 0.9844 - val_loss: 0.9995 - val_accuracy: 0.7583\n",
      "Epoch 13/15\n",
      "1054/1054 [==============================] - 93s 87ms/step - loss: 0.0590 - accuracy: 0.9848 - val_loss: 1.0871 - val_accuracy: 0.7365\n",
      "Epoch 14/15\n",
      "1054/1054 [==============================] - 115s 107ms/step - loss: 0.0490 - accuracy: 0.9865 - val_loss: 1.0015 - val_accuracy: 0.7596\n",
      "Epoch 15/15\n",
      "1054/1054 [==============================] - 140s 130ms/step - loss: 0.0548 - accuracy: 0.9850 - val_loss: 0.9614 - val_accuracy: 0.7802\n",
      "\n",
      "### Fold 3 ì‹œì‘ ###\n",
      "Epoch 1/15\n",
      "1054/1054 [==============================] - 132s 113ms/step - loss: 2.8131 - accuracy: 0.4515 - val_loss: 4.1377 - val_accuracy: 0.1980\n",
      "Epoch 2/15\n",
      "1054/1054 [==============================] - 120s 110ms/step - loss: 0.9002 - accuracy: 0.7479 - val_loss: 2.4628 - val_accuracy: 0.3798\n",
      "Epoch 3/15\n",
      "1054/1054 [==============================] - 136s 126ms/step - loss: 0.4108 - accuracy: 0.8784 - val_loss: 1.5483 - val_accuracy: 0.5605\n",
      "Epoch 4/15\n",
      "1054/1054 [==============================] - 95s 87ms/step - loss: 0.2544 - accuracy: 0.9238 - val_loss: 1.5569 - val_accuracy: 0.5827\n",
      "Epoch 5/15\n",
      "1054/1054 [==============================] - 113s 105ms/step - loss: 0.1900 - accuracy: 0.9415 - val_loss: 1.2009 - val_accuracy: 0.6637\n",
      "Epoch 6/15\n",
      "1054/1054 [==============================] - 91s 85ms/step - loss: 0.1470 - accuracy: 0.9550 - val_loss: 1.3097 - val_accuracy: 0.6612\n",
      "Epoch 7/15\n",
      "1054/1054 [==============================] - 92s 85ms/step - loss: 0.1198 - accuracy: 0.9646 - val_loss: 1.1324 - val_accuracy: 0.7120\n",
      "Epoch 8/15\n",
      "1054/1054 [==============================] - 92s 86ms/step - loss: 0.1012 - accuracy: 0.9696 - val_loss: 1.0537 - val_accuracy: 0.7381\n",
      "Epoch 9/15\n",
      "1054/1054 [==============================] - 94s 88ms/step - loss: 0.0946 - accuracy: 0.9733 - val_loss: 1.0276 - val_accuracy: 0.7474\n",
      "Epoch 10/15\n",
      "1054/1054 [==============================] - 91s 85ms/step - loss: 0.0768 - accuracy: 0.9776 - val_loss: 1.0920 - val_accuracy: 0.7417\n",
      "Epoch 11/15\n",
      "1054/1054 [==============================] - 93s 86ms/step - loss: 0.0768 - accuracy: 0.9775 - val_loss: 0.9197 - val_accuracy: 0.7802\n",
      "Epoch 12/15\n",
      "1054/1054 [==============================] - 92s 86ms/step - loss: 0.0639 - accuracy: 0.9820 - val_loss: 0.9473 - val_accuracy: 0.7718\n",
      "Epoch 13/15\n",
      "1054/1054 [==============================] - 92s 86ms/step - loss: 0.0594 - accuracy: 0.9834 - val_loss: 0.8583 - val_accuracy: 0.7963\n",
      "Epoch 14/15\n",
      "1054/1054 [==============================] - 92s 85ms/step - loss: 0.0513 - accuracy: 0.9860 - val_loss: 0.9469 - val_accuracy: 0.7951\n",
      "Epoch 15/15\n",
      "1054/1054 [==============================] - 92s 85ms/step - loss: 0.0497 - accuracy: 0.9871 - val_loss: 0.7847 - val_accuracy: 0.8177\n",
      "\n",
      "### Fold 4 ì‹œì‘ ###\n",
      "Epoch 1/15\n",
      "1054/1054 [==============================] - 99s 87ms/step - loss: 2.8555 - accuracy: 0.4477 - val_loss: 4.2124 - val_accuracy: 0.1891\n",
      "Epoch 2/15\n",
      "1054/1054 [==============================] - 94s 87ms/step - loss: 0.8952 - accuracy: 0.7497 - val_loss: 2.3741 - val_accuracy: 0.3955\n",
      "Epoch 3/15\n",
      "1054/1054 [==============================] - 94s 87ms/step - loss: 0.4151 - accuracy: 0.8767 - val_loss: 1.8299 - val_accuracy: 0.4903\n",
      "Epoch 4/15\n",
      "1054/1054 [==============================] - 93s 86ms/step - loss: 0.2593 - accuracy: 0.9212 - val_loss: 1.5547 - val_accuracy: 0.5696\n",
      "Epoch 5/15\n",
      "1054/1054 [==============================] - 93s 86ms/step - loss: 0.1969 - accuracy: 0.9409 - val_loss: 1.4577 - val_accuracy: 0.6143\n",
      "Epoch 6/15\n",
      "1054/1054 [==============================] - 94s 87ms/step - loss: 0.1502 - accuracy: 0.9553 - val_loss: 1.2876 - val_accuracy: 0.6569\n",
      "Epoch 7/15\n",
      "1054/1054 [==============================] - 94s 87ms/step - loss: 0.1199 - accuracy: 0.9652 - val_loss: 1.1306 - val_accuracy: 0.7139\n",
      "Epoch 8/15\n",
      "1054/1054 [==============================] - 94s 87ms/step - loss: 0.1048 - accuracy: 0.9690 - val_loss: 1.2849 - val_accuracy: 0.6825\n",
      "Epoch 9/15\n",
      "1054/1054 [==============================] - 94s 87ms/step - loss: 0.0885 - accuracy: 0.9748 - val_loss: 1.2489 - val_accuracy: 0.7087\n",
      "Epoch 10/15\n",
      "1054/1054 [==============================] - 94s 87ms/step - loss: 0.0829 - accuracy: 0.9765 - val_loss: 0.8858 - val_accuracy: 0.7822\n",
      "Epoch 11/15\n",
      "1054/1054 [==============================] - 94s 87ms/step - loss: 0.0698 - accuracy: 0.9801 - val_loss: 0.8994 - val_accuracy: 0.7908\n",
      "Epoch 12/15\n",
      "1054/1054 [==============================] - 94s 87ms/step - loss: 0.0630 - accuracy: 0.9822 - val_loss: 0.7813 - val_accuracy: 0.8137\n",
      "Epoch 13/15\n",
      "1054/1054 [==============================] - 94s 87ms/step - loss: 0.0598 - accuracy: 0.9831 - val_loss: 0.9443 - val_accuracy: 0.7851\n",
      "Epoch 14/15\n",
      "1054/1054 [==============================] - 94s 87ms/step - loss: 0.0582 - accuracy: 0.9842 - val_loss: 0.8080 - val_accuracy: 0.8098\n",
      "Epoch 15/15\n",
      "1054/1054 [==============================] - 94s 87ms/step - loss: 0.0504 - accuracy: 0.9862 - val_loss: 0.7782 - val_accuracy: 0.8114\n",
      "\n",
      "### Fold 5 ì‹œì‘ ###\n",
      "Epoch 1/15\n",
      "1054/1054 [==============================] - 105s 93ms/step - loss: 2.8531 - accuracy: 0.4486 - val_loss: 4.2469 - val_accuracy: 0.2092\n",
      "Epoch 2/15\n",
      "1054/1054 [==============================] - 97s 89ms/step - loss: 0.8696 - accuracy: 0.7602 - val_loss: 2.4968 - val_accuracy: 0.3657\n",
      "Epoch 3/15\n",
      "1054/1054 [==============================] - 99s 92ms/step - loss: 0.3994 - accuracy: 0.8834 - val_loss: 1.6235 - val_accuracy: 0.5309\n",
      "Epoch 4/15\n",
      "1054/1054 [==============================] - 96s 89ms/step - loss: 0.2484 - accuracy: 0.9263 - val_loss: 1.4474 - val_accuracy: 0.5953\n",
      "Epoch 5/15\n",
      "1054/1054 [==============================] - 94s 87ms/step - loss: 0.1827 - accuracy: 0.9450 - val_loss: 1.3266 - val_accuracy: 0.6477\n",
      "Epoch 6/15\n",
      "1054/1054 [==============================] - 95s 89ms/step - loss: 0.1369 - accuracy: 0.9585 - val_loss: 1.3588 - val_accuracy: 0.6612\n",
      "Epoch 7/15\n",
      "1054/1054 [==============================] - 96s 89ms/step - loss: 0.1202 - accuracy: 0.9643 - val_loss: 1.2802 - val_accuracy: 0.6849\n",
      "Epoch 8/15\n",
      "1054/1054 [==============================] - 97s 90ms/step - loss: 0.1045 - accuracy: 0.9693 - val_loss: 1.1700 - val_accuracy: 0.7090\n",
      "Epoch 9/15\n",
      "1054/1054 [==============================] - 96s 89ms/step - loss: 0.0865 - accuracy: 0.9760 - val_loss: 0.9177 - val_accuracy: 0.7606\n",
      "Epoch 10/15\n",
      "1054/1054 [==============================] - 94s 87ms/step - loss: 0.0778 - accuracy: 0.9795 - val_loss: 1.0610 - val_accuracy: 0.7373\n",
      "Epoch 11/15\n",
      "1054/1054 [==============================] - 95s 88ms/step - loss: 0.0688 - accuracy: 0.9813 - val_loss: 0.9762 - val_accuracy: 0.7588\n",
      "Epoch 12/15\n",
      "1054/1054 [==============================] - 95s 88ms/step - loss: 0.0668 - accuracy: 0.9824 - val_loss: 0.9550 - val_accuracy: 0.7715\n",
      "Epoch 13/15\n",
      "1054/1054 [==============================] - 93s 87ms/step - loss: 0.0611 - accuracy: 0.9831 - val_loss: 0.9997 - val_accuracy: 0.7664\n",
      "Epoch 14/15\n",
      "1054/1054 [==============================] - 94s 87ms/step - loss: 0.0558 - accuracy: 0.9846 - val_loss: 0.9775 - val_accuracy: 0.7740\n",
      "Epoch 15/15\n",
      "1054/1054 [==============================] - 96s 89ms/step - loss: 0.0438 - accuracy: 0.9877 - val_loss: 0.7636 - val_accuracy: 0.8148\n",
      "í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¶”ë¡  ì‹œì‘...\n",
      "345/345 [==============================] - 91s 262ms/step\n",
      "ì¶”ë¡  ì™„ë£Œ\n",
      "submission.csv ì €ì¥ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.applications.efficientnet import preprocess_input\n",
    "\n",
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# -----------------------\n",
    "# GPU í™•ì¸\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    print(f\"[INFO] {len(gpus)} GPU(s) available: {[gpu.name for gpu in gpus]}\")\n",
    "else:\n",
    "    print(\"[INFO] No GPU available. Training will use CPU.\")\n",
    "\n",
    "# -----------------------\n",
    "# Config\n",
    "CFG = {\n",
    "    'IMG_SIZE': 224,\n",
    "    'EPOCHS': 15,\n",
    "    'LR': 3e-4,\n",
    "    'BATCH_SIZE': 24,\n",
    "    'SEED': 2025,\n",
    "    'FOLDS': 5\n",
    "}\n",
    "\n",
    "# Seed ê³ ì •\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "\n",
    "set_seed(CFG['SEED'])\n",
    "\n",
    "# -----------------------\n",
    "# ê²½ë¡œ ì„¤ì •\n",
    "ORIGINAL_TRAIN_DIR = \"D:/ë°ì´ì½˜ 250519 ëŒ€íšŒ/open/train\"        # ì›ë³¸ ì›ì²œ ë°ì´í„°ì…‹\n",
    "FILTERED_TRAIN_DIR = \"D:/ë°ì´ì½˜ 250519 ëŒ€íšŒ/filtered_train\"    # í•„í„°ë§ ê²°ê³¼ ì €ì¥ í´ë”\n",
    "TEST_DIR = \"D:/ë°ì´ì½˜ 250519 ëŒ€íšŒ/open/test\"\n",
    "SAMPLE_SUB = \"D:/ë°ì´ì½˜ 250519 ëŒ€íšŒ/open/sample_submission.csv\"\n",
    "\n",
    "# -----------------------\n",
    "# YOLOv8 ì°¨ëŸ‰ ì™¸ê´€ í•„í„°ë§ í•¨ìˆ˜\n",
    "\n",
    "VEHICLE_CLASSES = [2, 5, 7]  # car, bus, truck í´ë˜ìŠ¤ ë²ˆí˜¸\n",
    "\n",
    "yolo_model = YOLO('yolov8n.pt')  # í•„ìš”ì‹œ yolov8s.pt ë“±ìœ¼ë¡œ ë³€ê²½ ê°€ëŠ¥\n",
    "\n",
    "def is_full_vehicle(detections, img):\n",
    "    h, w = img.shape[:2]\n",
    "    img_area = h * w\n",
    "\n",
    "    for det in detections:\n",
    "        cls = int(det.cls)\n",
    "        if cls not in VEHICLE_CLASSES:\n",
    "            continue\n",
    "\n",
    "        x1, y1, x2, y2 = det.xyxy[0].cpu().numpy()\n",
    "        box_w, box_h = x2 - x1, y2 - y1\n",
    "        area = box_w * box_h\n",
    "        center_x, center_y = (x1 + x2) / 2, (y1 + y2) / 2\n",
    "        aspect_ratio = box_w / box_h\n",
    "\n",
    "        # ì°¨ëŸ‰ ì¤‘ì•™ ê·¼ì²˜ ìœ„ì¹˜ ì¡°ê±´\n",
    "        if not (0.35 * w < center_x < 0.65 * w and 0.35 * h < center_y < 0.65 * h):\n",
    "            continue\n",
    "\n",
    "        # ì°¨ëŸ‰ ë°•ìŠ¤ í¬ê¸° ìµœì†Œ ë¹„ìœ¨ ì¡°ê±´\n",
    "        if area / img_area < 0.35:\n",
    "            continue\n",
    "\n",
    "        # ì°¨ëŸ‰ ë¹„ìœ¨ ì¡°ê±´ (íŠ¸ë í¬/ë³´ë‹› ì—´ë¦¼ ë“± ì œê±°ìš©)\n",
    "        if aspect_ratio < 0.8 or aspect_ratio > 2.5:\n",
    "            continue\n",
    "\n",
    "        # ìœ„ ì¡°ê±´ ëª¨ë‘ í†µê³¼ ì‹œ ì „ì²´ ì°¨ëŸ‰ ì™¸ê´€ í¬í•¨ìœ¼ë¡œ íŒë‹¨\n",
    "        return True\n",
    "\n",
    "    return False\n",
    "\n",
    "# -----------------------\n",
    "# ì°¨ëŸ‰ ì™¸ê´€ í•„í„°ë§ ìˆ˜í–‰\n",
    "\n",
    "print(\"ì°¨ëŸ‰ ì™¸ê´€ ì´ìƒì¹˜ í•„í„°ë§ ì‹œì‘...\")\n",
    "os.makedirs(FILTERED_TRAIN_DIR, exist_ok=True)\n",
    "class_dirs = sorted(os.listdir(ORIGINAL_TRAIN_DIR))\n",
    "\n",
    "filtered_count = 0\n",
    "total_count = 0\n",
    "\n",
    "for cls in tqdm(class_dirs, desc=\"í´ë˜ìŠ¤ë³„ í•„í„°ë§\"):\n",
    "    input_dir = os.path.join(ORIGINAL_TRAIN_DIR, cls)\n",
    "    output_dir = os.path.join(FILTERED_TRAIN_DIR, cls)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    image_paths = glob(os.path.join(input_dir, \"*.jpg\"))\n",
    "    for img_path in image_paths:\n",
    "        total_count += 1\n",
    "        img = cv2.imread(img_path)\n",
    "        results = yolo_model(img, verbose=False)[0]\n",
    "\n",
    "        if is_full_vehicle(results.boxes, img):\n",
    "            filename = os.path.basename(img_path)\n",
    "            cv2.imwrite(os.path.join(output_dir, filename), img)\n",
    "            filtered_count += 1\n",
    "\n",
    "print(f\"ğŸ” ì „ì²´ ì´ë¯¸ì§€ ìˆ˜: {total_count}\")\n",
    "print(f\"âœ… í•„í„°ë§ëœ ì°¨ëŸ‰ ì™¸ê´€ ì´ë¯¸ì§€ ìˆ˜: {filtered_count}\")\n",
    "print(f\"ğŸ§¼ ì´ìƒì¹˜ ì œê±° ì™„ë£Œ. ì •ì œ ë¹„ìœ¨: {filtered_count / total_count:.2%}\")\n",
    "\n",
    "# -----------------------\n",
    "# í•™ìŠµ ë°ì´í„° ì¤€ë¹„\n",
    "\n",
    "label_list = sorted(os.listdir(FILTERED_TRAIN_DIR))\n",
    "label2id = {v: i for i, v in enumerate(label_list)}\n",
    "id2label = {i: v for v, i in label2id.items()}\n",
    "\n",
    "image_paths = glob(os.path.join(FILTERED_TRAIN_DIR, '*', '*.jpg'))\n",
    "labels = [label2id[os.path.basename(os.path.dirname(p))] for p in image_paths]\n",
    "\n",
    "# -----------------------\n",
    "# ë°ì´í„° ì „ì²˜ë¦¬ í•¨ìˆ˜\n",
    "\n",
    "def load_and_preprocess(img_path):\n",
    "    img = load_img(img_path, target_size=(CFG['IMG_SIZE'], CFG['IMG_SIZE']))\n",
    "    img = img_to_array(img)\n",
    "    img = preprocess_input(img)\n",
    "    return img\n",
    "\n",
    "def create_dataset(image_paths, labels=None, is_train=True):\n",
    "    def gen():\n",
    "        for i, path in enumerate(image_paths):\n",
    "            img = load_and_preprocess(path)\n",
    "            if labels is not None:\n",
    "                yield img, labels[i]\n",
    "            else:\n",
    "                yield img\n",
    "\n",
    "    if labels is not None:\n",
    "        ds = tf.data.Dataset.from_generator(\n",
    "            gen,\n",
    "            output_types=(tf.float32, tf.int32),\n",
    "            output_shapes=((CFG['IMG_SIZE'], CFG['IMG_SIZE'], 3), ())\n",
    "        )\n",
    "    else:\n",
    "        ds = tf.data.Dataset.from_generator(\n",
    "            gen,\n",
    "            output_types=tf.float32,\n",
    "            output_shapes=(CFG['IMG_SIZE'], CFG['IMG_SIZE'], 3)\n",
    "        )\n",
    "\n",
    "    if is_train:\n",
    "        ds = ds.shuffle(1024)\n",
    "    ds = ds.batch(CFG['BATCH_SIZE']).prefetch(tf.data.AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "# -----------------------\n",
    "# ëª¨ë¸ ìƒì„±\n",
    "\n",
    "def build_model(num_classes):\n",
    "    base = tf.keras.applications.EfficientNetB0(\n",
    "        include_top=False,\n",
    "        input_shape=(CFG['IMG_SIZE'], CFG['IMG_SIZE'], 3),\n",
    "        weights='imagenet',\n",
    "        pooling='avg'\n",
    "    )\n",
    "    x = layers.Dense(num_classes, activation='softmax')(base.output)\n",
    "    model = models.Model(inputs=base.input, outputs=x)\n",
    "    return model\n",
    "\n",
    "# -----------------------\n",
    "# Stratified K-Fold í•™ìŠµ\n",
    "\n",
    "skf = StratifiedKFold(n_splits=CFG['FOLDS'], shuffle=True, random_state=CFG['SEED'])\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(image_paths, labels)):\n",
    "    print(f\"\\n### Fold {fold+1} ì‹œì‘ ###\")\n",
    "\n",
    "    train_paths = [image_paths[i] for i in train_idx]\n",
    "    val_paths = [image_paths[i] for i in val_idx]\n",
    "    train_labels = [labels[i] for i in train_idx]\n",
    "    val_labels = [labels[i] for i in val_idx]\n",
    "\n",
    "    train_ds = create_dataset(train_paths, train_labels, is_train=True)\n",
    "    val_ds = create_dataset(val_paths, val_labels, is_train=False)\n",
    "\n",
    "    model = build_model(num_classes=len(label2id))\n",
    "    model.compile(\n",
    "        optimizer=optimizers.Adam(CFG['LR']),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    model.fit(\n",
    "        train_ds,\n",
    "        validation_data=val_ds,\n",
    "        epochs=CFG['EPOCHS'],\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "# -----------------------\n",
    "# í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¶”ë¡  ë° ì œì¶œ íŒŒì¼ ìƒì„±\n",
    "\n",
    "test_paths = sorted(glob(os.path.join(TEST_DIR, '*.jpg')))\n",
    "test_ds = create_dataset(test_paths, is_train=False)\n",
    "\n",
    "print(\"í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¶”ë¡  ì‹œì‘...\")\n",
    "preds = model.predict(test_ds)\n",
    "print(\"ì¶”ë¡  ì™„ë£Œ\")\n",
    "\n",
    "submission = pd.read_csv(SAMPLE_SUB)\n",
    "for idx, class_name in enumerate(label2id.keys()):\n",
    "    submission[class_name] = preds[:, idx]\n",
    "\n",
    "submission.to_csv(\"submission.csv\", index=False)\n",
    "print(\"submission.csv ì €ì¥ ì™„ë£Œ\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
